{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 23:02:14 WARN Utils: Your hostname, yuzhuorandeMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.4.42 instead (on interface en0)\n",
      "24/03/20 23:02:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/20 23:02:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/20 23:02:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# Spark Session initialization\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select 100 stocks from Shanghai Stock Exchange(SSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Stock_ID|         Total_Value|\n",
      "+--------+--------------------+\n",
      "|  600004|   2.582089646753E10|\n",
      "|  600006|            1.282E10|\n",
      "|  600007|    2.11025690873E10|\n",
      "|  600008|2.011321845498000...|\n",
      "|  600009|9.135214999140001E10|\n",
      "|  600010|   7.173980875184E10|\n",
      "|  600011|   1.161658908566E11|\n",
      "|  600012|       1.71500274E10|\n",
      "|  600015|    9.07150922676E10|\n",
      "|  600016|  1.6768666286266E11|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_stock_df = spark.read.csv('Dim_Stock.csv', header=True, inferSchema=True)\n",
    "sse_stocks_df = dim_stock_df.filter(col(\"stock_id\") > 600000).orderBy(\"stock_id\").limit(10)\n",
    "sse_stocks_df.select('Stock_ID', 'Total_Value').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --select all stocks that rise over 9.9% on 2024-03-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+-----+------+-----+-------+---------------+---------+----------------------+---------------------+-------------+------+\n",
      "|        FSD_ID|Stock_ID| Date_ID| Open|  High|  Low| Volume|       Turnover|Amplitude|Percent_Of_Incre_Decre|Amount_Of_Incre_Decre|Turnover_Rate| Close|\n",
      "+--------------+--------+--------+-----+------+-----+-------+---------------+---------+----------------------+---------------------+-------------+------+\n",
      "|20240311300317|  300317|20240311| 4.04|  4.84| 4.04| 972902|   4.60648926E8|    19.85|                  20.1|                 0.81|        11.76|  4.84|\n",
      "|20240311300324|  300324|20240311| 2.98|  3.29| 2.88|1842843|   5.68963029E8|    14.96|                 20.07|                 0.55|        10.79|  3.29|\n",
      "|20240311300530|  300530|20240311| 17.8|  21.4| 17.8|  94640|   1.91813994E8|    20.19|                 20.02|                 3.57|         5.98|  21.4|\n",
      "|20240311301292|  301292|20240311|14.16| 16.85|14.16|  50492|    8.4292834E7|    19.16|                 20.01|                 2.81|        11.18| 16.85|\n",
      "|20240311301202|  301202|20240311| 28.7| 34.42|27.66| 136343|   4.30519547E8|    23.57|                 20.01|                 5.74|        39.98| 34.42|\n",
      "|20240311300890|  300890|20240311|23.37| 27.95|23.37|  76936|   2.03484159E8|    19.67|                 20.01|                 4.66|          7.9| 27.95|\n",
      "|20240311301349|  301349|20240311|28.89| 34.66|28.89|  30457|   1.01086693E8|    19.98|                 20.01|                 5.78|         7.23| 34.66|\n",
      "|20240311300438|  300438|20240311|22.01| 25.91|22.01| 425121|  1.021499595E9|    18.06|                 20.01|                 4.32|        11.77| 25.91|\n",
      "|20240311688063|  688063|20240311| 85.0|  99.6| 85.0| 136891|   1.26584976E9|    17.59|                  20.0|                 16.6|         7.79|  99.6|\n",
      "|20240311300712|  300712|20240311|21.48| 25.14|21.37|  92134|   2.19739943E8|     18.0|                  20.0|                 4.19|         4.94| 25.14|\n",
      "|20240311301205|  301205|20240311| 91.0|116.62| 91.0| 133827|  1.369576675E9|    26.36|                 17.31|                16.82|        19.69| 114.0|\n",
      "|20240311300025|  300025|20240311| 8.54| 10.08| 8.38| 306597|   2.85149119E8|    20.09|                 17.14|                 1.45|         7.07|  9.91|\n",
      "|20240311300769|  300769|20240311|41.01| 46.89|41.01| 302334|  1.342825861E9|    14.71|                 16.59|                 6.63|        12.05|  46.6|\n",
      "|20240311300953|  300953|20240311| 45.8|  53.0| 45.8|  31804|   1.59406988E8|    15.72|                 15.31|                 7.01|         6.98|  52.8|\n",
      "|20240311688248|  688248|20240311|27.78| 31.31|27.78| 171501|   5.19394678E8|    13.07|                 15.07|                 4.07|         7.51| 31.07|\n",
      "|20240311300750|  300750|20240311|165.0|181.65|164.1| 763350|1.3248182793E10|    11.11|                 14.46|                22.85|         1.96|180.85|\n",
      "|20240311301236|  301236|20240311|46.95| 55.88| 46.8|1141301|  5.880365894E9|    18.92|                  14.4|                 6.91|        16.78| 54.91|\n",
      "|20240311300709|  300709|20240311| 38.8| 44.98|36.89| 446070|  1.808201963E9|     21.3|                 14.01|                 5.32|        29.96|  43.3|\n",
      "|20240311300014|  300014|20240311| 38.6| 42.76|38.55| 648947|  2.651262664E9|    11.19|                  13.0|                 4.89|         3.49|  42.5|\n",
      "|20240311300411|  300411|20240311| 8.12|  9.49| 8.12| 962847|   8.43633186E8|    17.19|                 12.92|                 1.03|        35.63|   9.0|\n",
      "+--------------+--------+--------+-----+------+-----+-------+---------------+---------+----------------------+---------------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_stock_daily_df = spark.read.csv('Fact_Stock_Daily_2024.csv', header=True, inferSchema=True)\n",
    "rising_stocks_df = fact_stock_daily_df.filter((col(\"Date_ID\") == 20240311) & (col(\"percent_of_incre_decre\") > 9.9))\\\n",
    "                    .orderBy(col(\"percent_of_incre_decre\").desc())\n",
    "rising_stocks_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select information from 20231120 9:30-9:40 on stock SH600004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+------------+----+-----+-----+------+----------+---------+-----+\n",
      "|            FSM_ID|Stock_ID|   Minute_ID|Open| High|  Low|Volume|Stock_Name| Turnover|Close|\n",
      "+------------------+--------+------------+----+-----+-----+------+----------+---------+-----+\n",
      "|600004202311200931|  600004|202311200931| 0.0|10.92|10.89|   647|   ���ƻ���| 705561.0|10.91|\n",
      "|600004202311200932|  600004|202311200932| 0.0| 10.9|10.89|   401|   ���ƻ���| 436909.0|10.89|\n",
      "|600004202311200933|  600004|202311200933| 0.0|10.91|10.86|  2568|   ���ƻ���|2794153.0| 10.9|\n",
      "|600004202311200934|  600004|202311200934| 0.0|10.91| 10.9|   310|   ���ƻ���| 338083.0| 10.9|\n",
      "|600004202311200935|  600004|202311200935| 0.0|10.95|10.91|  1727|   ���ƻ���|1886825.0|10.94|\n",
      "|600004202311200936|  600004|202311200936| 0.0|10.94|10.92|   202|   ���ƻ���| 220870.0|10.93|\n",
      "|600004202311200937|  600004|202311200937| 0.0|10.94|10.93|  1230|   ���ƻ���|1345150.0|10.93|\n",
      "|600004202311200938|  600004|202311200938| 0.0|10.93|10.93|   211|   ���ƻ���| 230657.0|10.93|\n",
      "|600004202311200939|  600004|202311200939| 0.0|10.94|10.93|    86|   ���ƻ���|  94037.0|10.94|\n",
      "+------------------+--------+------------+----+-----+-----+------+----------+---------+-----+\n",
      "\n",
      "Time used: 4.820951700210571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time=  time.time()\n",
    "fact_stock_minute_df = spark.read.csv('Fact_Stock_Minute.csv', header=True, inferSchema=True)\n",
    "stock_600004_info_df = fact_stock_minute_df.filter((col(\"Minute_ID\") > 202311200930) \\\n",
    "                                                   & (col(\"Minute_ID\") < 202311200940) \\\n",
    "                                                    & (col(\"Stock_ID\") == 600004))\n",
    "stock_600004_info_df.show()\n",
    "print(f\"Time used: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show total minutes in 2024-03-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Total_Minutes|\n",
      "+-------------+\n",
      "|          241|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_minute_df = spark.read.csv('Dim_Minute_2024.csv', header=True, inferSchema=True)\n",
    "total_minutes_df = dim_minute_df.filter(col(\"Date\").cast(\"date\") == \"2024-03-11\")\\\n",
    "                                .agg({\"Minute_ID\": \"count\"})\\\n",
    "                                .withColumnRenamed(\"count(Minute_ID)\", \"Total_Minutes\")\n",
    "total_minutes_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show total trading days in Feburary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Total_Trading_Days|\n",
      "+------------------+\n",
      "|                16|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_date_df = spark.read.csv('Dim_Date_2024.csv', header=True, inferSchema=True)\n",
    "total_trading_days_df = dim_date_df.filter((col(\"year\") == 2024) & (col(\"month\") == 2))\\\n",
    "    .agg({\"Date_ID\": \"count\"})\\\n",
    "    .withColumnRenamed(\"count(Date_ID)\", \"Total_Trading_Days\")\n",
    "total_trading_days_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show recent company events from September (company code between 600000 and 600010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+--------+-------------------+------------+-------------------+----------------+\n",
      "|            FCE_ID|FCE_Stock_ID| Date_ID|         Event_Type|Current_Flag|Effective_Timestamp|Expire_Timestamp|\n",
      "+------------------+------------+--------+-------------------+------------+-------------------+----------------+\n",
      "|202309150082600006|      600006|20230915|Asset Restructuring|           1|         2023-09-15|            NULL|\n",
      "|202311140063600008|      600008|20231114|Asset Restructuring|           1|         2023-11-14|            NULL|\n",
      "+------------------+------------+--------+-------------------+------------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_company_event_df = spark.read.csv('Fact_Company_Event.csv', header=True, inferSchema=True).withColumnRenamed(\"Stock_ID\",\"FCE_Stock_ID\")\n",
    "dim_event_df = spark.read.csv('Dim_Event.csv', header=True, inferSchema=True).withColumnRenamed(\"Stock_ID\",\"DE_Stock_ID\")\n",
    "\n",
    "recent_events_df = fact_company_event_df.join(dim_event_df, fact_company_event_df.Event_Dim_ID == dim_event_df.Event_Dim_ID, \"inner\")\\\n",
    "                    .select(\"FCE_ID\",\"FCE_Stock_ID\",\"Date_ID\",\"Event_Type\",\"Current_Flag\",\"Effective_Timestamp\",\"Expire_Timestamp\")\\\n",
    "                    .filter((col(\"Date_ID\") > 20230901) & (col(\"FCE_Stock_ID\") > 600000) & (col(\"FCE_Stock_ID\") < 600010))\n",
    "                    \n",
    "recent_events_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select Minute Level Data from 2023-11-21 9:30 to 2023-11-21 9:40 where the company is in CN market and its Total_Value is in top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+------------+-----+----+----+------+\n",
      "|            FSM_ID|Stock_ID|   Minute_ID|Close|High| Low|volume|\n",
      "+------------------+--------+------------+-----+----+----+------+\n",
      "|601288202311210930|  601288|202311210930| 3.66|3.66|3.66| 11861|\n",
      "|601288202311210931|  601288|202311210931| 3.66|3.67|3.65|155799|\n",
      "|601288202311210932|  601288|202311210932| 3.66|3.67|3.65|116492|\n",
      "|601288202311210933|  601288|202311210933| 3.67|3.67|3.66| 11392|\n",
      "|601288202311210934|  601288|202311210934| 3.66|3.67|3.66| 14070|\n",
      "|601288202311210935|  601288|202311210935| 3.67|3.67|3.66| 28420|\n",
      "|601288202311210936|  601288|202311210936| 3.66|3.67|3.66| 14610|\n",
      "|601288202311210937|  601288|202311210937| 3.67|3.67|3.66|  9740|\n",
      "|601288202311210938|  601288|202311210938| 3.66|3.67|3.66|  9401|\n",
      "|601288202311210939|  601288|202311210939| 3.66|3.67|3.66| 10829|\n",
      "|601288202311210940|  601288|202311210940| 3.67|3.67|3.66| 38237|\n",
      "|601857202311210930|  601857|202311210930| 7.14|7.14|7.14|  4878|\n",
      "|601857202311210931|  601857|202311210931| 7.15|7.15|7.12| 23906|\n",
      "|601857202311210932|  601857|202311210932| 7.12|7.14|7.12|  9806|\n",
      "|601857202311210933|  601857|202311210933| 7.12|7.13|7.11|  4412|\n",
      "|601857202311210934|  601857|202311210934| 7.14|7.14|7.12|  5834|\n",
      "|601857202311210935|  601857|202311210935| 7.15|7.15|7.13| 14307|\n",
      "|601857202311210936|  601857|202311210936| 7.14|7.15|7.14|  7101|\n",
      "|601857202311210937|  601857|202311210937| 7.14|7.15|7.14|  2217|\n",
      "|601857202311210938|  601857|202311210938| 7.15|7.15|7.14|  4517|\n",
      "+------------------+--------+------------+-----+----+----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time used: 0.6944880485534668\n"
     ]
    }
   ],
   "source": [
    "start_time=  time.time()\n",
    "top_10_cn_stocks_cached = dim_stock_df.filter(col(\"Stock_Market\") == \"CN\") \\\n",
    "    .orderBy(col(\"Total_Value\").desc()) \\\n",
    "    .limit(10) \\\n",
    "    .select(\"Stock_ID\") \\\n",
    "    .withColumnRenamed(\"Stock_ID\",\"top10_Stock_ID\")\\\n",
    "    .cache()\n",
    "\n",
    "\n",
    "minute_level_data_df = fact_stock_minute_df.join(top_10_cn_stocks_cached, \\\n",
    "                        fact_stock_minute_df.Stock_ID == top_10_cn_stocks_cached.top10_Stock_ID, \"inner\") \\\n",
    "    .filter((col(\"Minute_ID\") >= 202311210930) & (col(\"Minute_ID\") <= 202311210940)) \\\n",
    "    .select(\"FSM_ID\", \"Stock_ID\", \"Minute_ID\", \"Close\", \"High\", \"Low\", \"volume\")\n",
    "\n",
    "minute_level_data_df.show()\n",
    "\n",
    "top_10_cn_stocks_cached.unpersist()\n",
    "print(f\"Time used: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
